The paper introduces a new file system, The Google File System, which is a scalable and distributed designed and implemented for large, distributed data-intensive applications. It supports fault tolerance and runs on inexpensive commodity hardware. It delivers high performance to a large number of clients. The paper discusses the GFS's interface extensions and benchmark results.

GFS provides a familiar file system interface, though it does not implement a standard API. Files are organized hierarchically in directories and identified by path names. Usual operations to create, delete, open, close, read, and write files are supported. Moreover, GFS has snapshot and record append operations. Snapshot creates a copy of a file or a directory tree at low cost. Record append allows multiple clients to append data to the same file concurrently while guaranteeing the atomicity of each individual clientâ€™s append. It is useful for implementing multi-way merge results and producer consumer queues that many clients can simultaneously append to without additional locking. 

A GFS cluster consists of a single master and multiple chunkservers and is accessed by multiple clients.Each of these is typically a commodity Linux machine running a user-level server process. It is easy to run both a chunkserver and a client on the same machine, as long as machine resources permit and the lower reliability caused by running possibly flaky application code is acceptable. Files are divided into fixed-size chunks. Each chunk is identified by an immutable and globally unique 64 bit chunk handle assigned by the master at the time of chunk creation. Chunkservers store chunks on local disks as Linux files and read or write chunk data specified by a chunk handle and byte range. For reliability, each chunk is replicated on multiple chunkservers. 

GFS has a relaxed consistency model that supports highly distributed applications well but remains relatively simple and efficient to implement. The GFS uses only replication for redundancy and so consumes more raw storage. This is due to fact that disks are cheap these days.

This paper and its implementation may be useful to our research if we end up using a new file system in order to increase the performance. I suppose we will need to investigate the compression first, then column-store next, and using a new file systems maye come after that. So, this paper has less significance to our research than most of the other papers. 

